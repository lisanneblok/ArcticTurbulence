{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge all datasets\n",
    "Arctic dataframe and dataframe from Waterhouse et al., made in data_exploration/load_ds notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "from src.utils.directories import get_parent_directory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_dir = get_parent_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_pkl = os.path.join(parent_dir, \"data/processed/Mashayek2022/input_microstructure.pkl\")\n",
    "global_df = pd.read_pickle(global_pkl)\n",
    "global_df = global_df.rename(columns={'lat': 'latitude', 'log_N2_sort': 'log_N2'})\n",
    "\n",
    "# Rename columns using the rename() method\n",
    "waterhouse_df = global_df.rename(columns={'log_N2_sort': 'log_N2', 'log_kappa': 'LK', 'lat': 'latitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# arctic_pkl = '/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/ml_ready/arctic2_1805.pkl'\n",
    "arctic_pkl = (os.path.join(parent_dir, \"data/processed/ml_ready/1710_time.pkl\"))\n",
    "arctic_df = pd.read_pickle(arctic_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Concatenate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([arctic_df, waterhouse_df], axis=0)\n",
    "arctic_df = all_df[['depth', 'profile', 'cruise', 'latitude', 'longitude',\n",
    "                 'S', 'T', 'log_eps', 'log_N2', 'dTdz', 'dSdz', 'time', 'season', 'seasonal_sin']]\n",
    "all_df = all_df[['depth', 'profile', 'cruise', 'latitude',\n",
    "                 'S', 'T', 'log_eps', 'log_N2', 'dTdz', 'dSdz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round depth down to the nearest integer\n",
    "all_df['rounded_depth'] = np.abs(np.floor(all_df['depth']).astype(int))\n",
    "\n",
    "# Filter the DataFrame to include only values that are multiples of ten\n",
    "filtered_df = all_df[all_df['rounded_depth'] % 10 == 0]\n",
    "\n",
    "# Group the DataFrame by cruise\n",
    "grouped_df = filtered_df.groupby('cruise')\n",
    "\n",
    "# Create an empty DataFrame to store the filtered data\n",
    "filtered_df_1500 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each group (cruise) in the grouped DataFrame\n",
    "for cruise, group in grouped_df:\n",
    "    # Check if the group has more than 1500 points\n",
    "    if len(group) > 1500:\n",
    "        # Randomly select 1500 points from the group\n",
    "        selected_points = group.sample(n=1500, random_state=42)  # Adjust the random_state if desired\n",
    "        \n",
    "        # Append the selected points to the filtered DataFrame\n",
    "        filtered_df_1500 = pd.concat([filtered_df_1500, selected_points])\n",
    "    else:\n",
    "        # If the group has less than or equal to 1500 points, append all points\n",
    "        filtered_df_1500 = pd.concat([filtered_df_1500, group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1500[\"depth\"] = filtered_df_1500[\"rounded_depth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arctic_pkl_path = (os.path.join(parent_dir, \"data/processed/ml_ready/merged_arctic.pkl\"))\n",
    "arctic_df.to_pickle(arctic_pkl_path)\n",
    "\n",
    "all_pkl_path = (os.path.join(parent_dir, \"data/processed/ml_ready/merged_all.pkl\"))\n",
    "all_df.to_pickle(all_pkl_path)\n",
    "\n",
    "filtered_df_1500_path = (os.path.join(parent_dir, \"data/processed/ml_ready/all_1500.pkl\"))\n",
    "filtered_df_1500.to_pickle(filtered_df_1500_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
